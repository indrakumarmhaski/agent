/// <reference types="node" />
import BaseRetrieveData from './base-retrieve-data';
import stream from 'stream';
import { RetrieveRequestBulk1ById } from '../../interfaces';
export default class Bulk1ByIdRetrieveData extends BaseRetrieveData<stream.Readable | null> {
    private readonly objectName;
    private readonly api;
    private readonly pollInterval;
    private readonly contentType;
    private readonly maxParallelBatches;
    private readonly jobOperation;
    private readonly abortController;
    private readonly axiosConfig;
    private jobId;
    private chunkSize;
    private chunks;
    private error;
    private completedBatches;
    private batchResults;
    private failedIds;
    private isAllBatchesSubmitted;
    private submittedBatchesById;
    private get submittedBatches();
    constructor({ objectName, queryBuilder, pollInterval, jobOperation, instance, chunkSize, maxParallelBatches, contentType, ids, api, logger, }: RetrieveRequestBulk1ById);
    execute(): Promise<stream.Readable | null>;
    private getResultsIterator;
    private executeBatches;
    private setError;
    handleBatchResults(batchId: string, chunk: string[]): Promise<void>;
    /**
     * Keeping ids (`chunk`) for retry only when batch retried more than 30 times.
     *
     * [Bulk queries processing in Salesforce Docs](https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/asynch_api_bulk_query_processing.htm)
     * @param stateMessage all error messages from the batch, concatenated via ';'.
     * @param chunk ids used in batch.
     * */
    private handleFailedBatch;
    private handleFailedIds;
    private createJob;
    private createBatch;
    private waitForBatchCompletion;
    private fetchBatchResultIds;
    private fetchBatchResult;
    closeJob(): Promise<void>;
}
